# 30-Day Data Science Challenge

Welcome to the 30-Day Data Science Challenge repository!

## Introduction
This repository contains the code and resources for the 30-day data science challenge. Each day, we will tackle a mini data science project to sharpen our skills and explore various aspects of data analysis, machine learning, and more.

## Project Overview
### Day 1: Exploring Seabed Sonar Data
- **Objective:** Differentiate rocks from mines using machine learning on sonar signals.
- **Techniques:** Data preprocessing, machine learning modeling.
- **Tools:** Python, Jupyter Notebooks, scikit-learn.
- **Repository Content:** [Day 1 Folder](Day-01)
  
### Day 2: Fake News Detection Using Python & Machine Learning
- **Objective:** Develop a model to differentiate between real and fake news articles using Python and machine learning.
- **Techniques:** Natural Language Processing (NLP), Feature engineering, Machine learning classification
- **Tools:** Python, Jupyter Notebooks, scikit-learn, NLTK (Natural Language Toolkit)
- **Repository Content:** [Day 2 Folder](Day-02)

### Day 3: House Price Prediction Using XGBoost Regression
- **Objective:** Fine-tune and evaluate the XGBoost regression model to predict house prices accurately.
- **Techniques:** Hyperparameter tuning, Model evaluation, Feature importance analysis.
- **Tools:** Python, Jupyter Notebooks, scikit-learn, XGBoost.
- **Repository Content:** [Day 3 Folder](Day-03)

### Day 4: Credit Card Fraud Detection 
- **Objective:** Implement machine learning algorithms to accurately identify fraudulent credit card transactions.
- **Techniques:** Utilizing data preprocessing for handling imbalanced data and outliers, applying supervised learning algorithms such as Random Forest and Logistic Regression, evaluating models with precision, recall, and F1-score, and incorporating advanced anomaly detection techniques.
- **Tools:** Python, Jupyter Notebooks, scikit-learn, Pandas, NumPy.
- **Repository Content:** [Day 4 Folder](Day-04)
- **Data Source:** [data](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud?resource=download/)

### Day 5: Image Data Processing for Deep Learning
- **Objective:** Harness the power of Python to preprocess image data for deep learning applications.
- **Techniques:** Image preprocessing techniques for scaling, normalization, and augmentation; utilizing deep learning frameworks like TensorFlow and PyTorch for efficient image processing; evaluating model performance with accuracy, precision, recall, and confusion matrices; exploring advanced applications in computer vision and image recognition.
- **Tools:** Python, Jupyter Notebooks, TensorFlow, PyTorch, OpenCV, scikit-image.
- **Repository Content:** [Day 5 Folder](Day-05)

### Day 6: Sentiment Analysis with LSTM Networks on IMDB Reviews
- **Objective:** Unleash the potential of Long Short-Term Memory (LSTM) networks for sentiment analysis on IMDB movie reviews.
- **Techniques:** Data preprocessing: Cleaning, tokenization, and padding sequences.
Model building: Constructing LSTM networks with embedding layers and configuring hyperparameters.
Model training and evaluation: Splitting data, monitoring performance, and assessing accuracy with metrics like loss and accuracy.
- **Tools:** Python, Jupyter Notebooks, TensorFlow/Keras, NLTK.
- **Repository Content:** [Day 6 Folder](Day-06)

### Day 7: Building a Conversational Chatbot using OpenAI's GPT-4 API with Streamlit 
- **Objective:** Create a conversational chatbot using OpenAI's GPT-4 API, seamlessly integrated into a Streamlit web app to showcase the potential of GPT-4 in generating human-like responses and the simplicity of Streamlit for building interactive web applications. 
- **Techniques:** API Integration: Connecting to the OpenAI GPT-4 API.
Web App Development: Using Streamlit to create an interactive user interface.
Conversation Management: Handling user inputs and generating dynamic responses.
Deployment: Running the web app locally or deploying it to a cloud service
- **Tools:** Python, Streamlit, OpenAI GPT-4 API
- **Repository Content:** [Day 7 Folder](Day-07)

### Day 8: Bank Loan Prediction
- **Objective:** Develop a web app to predict bank loan approvals using historical data and machine learning.
- **Techniques:** Data Preprocessing: Handle missing values, encode categorical data, scale features.
Machine Learning: Train and evaluate a Logistic Regression model.
Web Development: Create an interactive user interface with Streamlit.
- **Tools:** Python, Pandas, Scikit-learn, Streamlit.
- **Repository Content:** [Day 8 Folder](Day-08)

### Day 9: Predicting Breast Cancer with Neural Networks in PyTorch 
- **Objective:** Delve into predicting breast cancer using neural networks with PyTorch in our latest project.
- **Techniques:** Data Preprocessing: Handle and prepare the data for training.
Model Architecture: Design and implement an effective neural network.
Training: Train the model for accurate predictions.
Evaluation: Assess the model's performance.
- **Tools:** Python, PyTorch.
- **Repository Content:** [Day 9 Folder](Day-09)

### Day 10: Chatbot with Google's Gemini Pro API using Streamlit 
- **Objective:** This project builds a chatbot web app using Google's Gemini Pro API for conversation and Streamlit for the user interface.
- **Techniques:** API Integration: Connecting to Google's Gemini Pro API for robust conversational capabilities.
Building the Web App: Setting up Streamlit for a responsive and interactive user interface.
User Experience Design: Ensuring smooth and intuitive interactions within the chatbot interface.
- **Tools:** Python, Streamlit, Google's Gemini Pro API, Requests Library.
- **Repository Content:** [Day 10 Folder](Day-10)

### Day 11: Dog vs Cat Classification System using Transfer Learning 
- **Objective:** This project aims to classify dogs and cats in images using transfer learning. A pre-trained MobileNet V2 model will be used for fast and accurate image classification.
- **Techniques:** This project builds a dog vs cat classifier using a pre-trained MobileNet V2 for speed and efficiency. It involves data prep (resizing, normalization, augmentation), fine-tuning the model, and evaluating its performance with various metrics.
- **Tools:** Python, TensorFlow/Keras, MobileNet V2, NumPy and Pandas, Matplotlib for Visualization.
- **Repository Content:** [Day 11 Folder](Day-11)

## Getting Started
To get started with the challenge:
1. Clone this repository to your local machine.
2. Navigate to the folder of the current day's project.
3. Follow the instructions provided in the project's README to set up your environment and run the code.

## Contributions
Contributions are welcome! If you have suggestions for improvements, new project ideas, or would like to contribute code, feel free to submit a pull request.

## Acknowledgments
A big thank you to everyone participating in this challenge! Let's learn, grow, and conquer the world of data science together!

## Connect with Us
- LinkedIn: [Your LinkedIn Profile](https://www.linkedin.com/in/anchal-singh-tanwar-858b21228/)

